---
title: "Parquet & Feather: Security Telemetry Benchmark"
authors:
  - dispanser
  - mavam
date: 2022-10-15
tags: [benchmark, arrow, parquet, feather, quarto, r]
---

How does Apache [Parquet][parquet] compare to [Feather][feather] for storing
structured security data? In this blog post, we answer this question.

[parquet]: https://parquet.apache.org/
[feather]: https://arrow.apache.org/docs/python/feather.html

<!--truncate-->

:::info Parquet & Feather: 2/3
This is blog post is part of a 3-piece series on Parquet and Feather.

1. [Enabling Open Investigations][parquet-and-feather-1]
2. Security Telemetry Benchmark (this blog post)
3. TBD

[parquet-and-feather-1]: /blog/parquet-and-feather-enabling-open-investigations/
:::

In the [previous blog post][parquet-and-feather-1], we explained why Parquet and
Feather are great building blocks for modern investigations. We now want to take
a closer look at how they actually perform with respect to the following
metrics:

- **Size**: how much space does typical security telemetry occupy?
- **Write throughput**: how fast can we write out to a store?

Parquet and Feather have different goals. While Parquet is an on-disk format
that optimizes for size, Feather is just a thin layer around the native Arrow
in-memory representation. This puts them at different points in the spectrum of
throughput and latency. To better understand this spectrum for real-world data,
we instrumented the write path of VAST, which consists roughly of the following
steps:

1. Parse the input
2. Convert it into Arrow record batches
3. Ship Arrow record batches to a VAST server
4. Write Arrow out into a Parquet or Feather store

Since steps (1â€“3) are the same for both stores, we ignore them in the following
analysis and solely zoom in on (4). For our measurements, we collect the
following data points:

- **Store**: the type of store plugin used in the measurement, i.e., `parquet`
  or `feather`.
- **Construction time**: the time it takes to convert Arrow record batches into
  Parquet or Feather. We fenced the corresponding code blocks and computed the
  difference in nanoseconds.
- **Input size**: the number of bytes that the to-be-converted record batches
  consume.
- **Output size**: the number of bytes that the store file takes up.
- **Number of events**: the total number of events in all input record batches
- **Number of record batches**
- **Schema**: the name of the schema; there exists one store file per schema
- **Zstd compression level**: the applied Zstd compression level 

## Dataset

The dataset used in our experimental setup is a cross between a "normal day in
a corporate network" and a set of malware attacks, found on
[malware-traffic-analysis.net][malware-traffic-analysis].

TODO: I wasn't able to find a reference to M57 online, which would be nice.

The idea is to create a large body of innocent-looking, regular network
traffic, playing the role of background noise, for a set of common and
well-known attack scenarios.

To make this look somewhat realistic, the individual datasets' time stamps have
been shifted to pretend them happening all on the same day using
[`wireshark`][wireshark], in particular [`editcap`][editcap]. The resulting
pcap files are merged into one big file using [`mergecap`][mergecap].

The resulting pcap files were additionally processed using [zeek][zeek] and
[suricata][suricata], providing three distinct data sources with 
35 different output schemas for our ingestion experiments:

+------------------+-------+
|            schema| events|
+------------------+-------+
|       pcap.packet|6349174|
|  suricata.anomaly|   4190|
|        suricata.d| 144560|
|   suricata.dcerpc|  17116|
|     suricata.dhcp|    324|
|      suricata.dns| 144560|
| suricata.fileinfo|  17982|
|     suricata.flow| 564873|
|      suricata.ftp|    544|
|     suricata.http|  75367|
|    suricata.ikev2|      1|
|     suricata.krb5|   1689|
|      suricata.sip|    468|
|      suricata.smb|  32384|
|     suricata.smtp|    405|
|     suricata.snmp|    144|
|     suricata.tftp|     31|
|      suricata.tls|  41465|
|         zeek.conn| 583838|
|            zeek.d|  90013|
|      zeek.dce_rpc|  19585|
|         zeek.dhcp|    267|
|          zeek.dns|  90013|
|          zeek.dpd|    870|
|        zeek.files|  21847|
|          zeek.ftp|      4|
|         zeek.http|  75290|
|     zeek.kerberos|   2708|
|         zeek.ntlm|    429|
|          zeek.ntp|   1224|
|         zeek.ocsp|   2874|
|zeek.packet_filter|      1|
|           zeek.pe|    315|
|       zeek.radius|      1|
|     zeek.reporter|      7|
|          zeek.sip|    565|
|    zeek.smb_files|   1059|
|  zeek.smb_mapping|   1584|
|         zeek.smtp|   1967|
|         zeek.snmp|    132|
|          zeek.ssl|  41372|
|       zeek.tunnel|    606|
|        zeek.weird|   4965|
|         zeek.x509|   2575|
+------------------+-------+

TODO: this table was generated with spark via
`data.groupBy('schema).agg(max('num_events).alias("events")).orderBy('schema).show`. 
Should be embedded w/ R, maybe? - Or, not at all as it's quite long and not telling much?

[malware-traffic-analysis]: https://www.malware-traffic-analysis.net/index.html
[wireshark]: https://www.wireshark.org
[editcap]: https://www.wireshark.org/docs/wsug_html_chunked/AppToolseditcap.html
[mergecap]: https://www.wireshark.org/docs/wsug_html_chunked/AppToolsmergecap.html
[zeek]: https://zeek.org/
[suricata]: https://suricata.io/

```{r}
#| code-fold: true
#| label: prepare-data
library(dplyr)
library(ggplot2)
library(lubridate)
library(scales)
library(tidyr)

theme_set(theme_minimal())

data <- read.csv("data.csv") |>
  rename(store = store_type) |>
  mutate(duration = dnanoseconds(duration))

original <- read.csv("sizes.csv") |>
  mutate(store = "original", store_class = "original") |>
  select(store, store_class, schema, bytes)

# Normalize store sizes by number of events/store.
normalized <- data |>
  mutate(duration_normalized = duration / num_events,
         bytes_memory_normalized = bytes_memory / num_events,
         bytes_storage_normalized = bytes_in_storage / num_events,
         bytes_ratio = bytes_in_storage / bytes_memory)

# Compute average over measurements.
aggregated <- normalized |>
  group_by(store, schema, zstd.level) |>
  summarize(duration = mean(duration_normalized),
            memory = mean(bytes_memory_normalized),
            storage = mean(bytes_storage_normalized))

# Treat in-memory measuremts as just another storage type.
memory <- aggregated |>
  filter(store == "feather" & zstd.level == 1) |>
  mutate(store = "memory", store_class = "memory") |>
  select(store, store_class, schema, bytes = memory)

# Unite with rest of data.
unified <- aggregated |>
  select(-memory) |>
  rename(bytes = storage, store_class = store) |>
  unite("store", store_class, zstd.level, sep = "+", remove = FALSE)
```

### Batching

When working with security event data, we see highly variable data shapes. Some
event types completely cominate a dataset, and the long tail is large.

On the inside, a store is just a concatenation of Arrow record batches. This
means the "batching shape" of the store is defined by two factors: number of
events per batch and total number of batches. Let's plot these against each
other to get a feel of the data:

```{r}
#| code-fold: true
#| label: events
no_segment <- data |>
  filter(zstd.level == 1) |>
  filter(store != "segment")

no_segment |>
  ggplot(aes(x = num_events, y = num_slices, size = bytes_in_storage, color = store)) +
    geom_point(alpha = 0.1) +
    scale_size(range = c(2, 10), labels = scales::label_bytes(units = "auto_si")) +
    scale_x_log10(labels = scales::label_comma()) +
    labs(color = "Store", size = "Size", x = "Events", y = "Batches")
```

Each point in the above plot represents one store file. The x-axis (log-scaled)
represents the number of events in the store and the y-axis the number of
batches. The point diameter represents the size of the store file on disk. The
color denotes the type of store: Parquet or Feather. We omit our Segment store,
as it's almost identical to Feather.

```{r}
#| echo: false
top <- no_segment |> arrange(num_events) |> tail(1)
bot <- no_segment |> arrange(num_events) |> head(1)
```

Our dataset has everything from a few dozons to millions of events per store.
The event "`r top |> pull(schema)`" contains a total of
`r top |> pull(num_events) |> label_comma()()` events, whereas the event
"`r bot |> pull(schema)`" has only `r bot |> pull(num_events)`. Parquet files
also have more batches compared to Feather.

:::caution TODO
Thomas, why? :-)

I don't think this is true anymore, I spot-checked a bunch of schemas and `num_slices`
is consistent across those.
:::

This high variance in number of events per store is suboptimal, because every
partition (that contains the store) has a unique entry and indexes in the
catalog. This causes fragmentation. Ideally, we would like to create a uniform
number of events per partition. Thankfully, VAST users do not have to worry
about this churn. As of [v2.3](/blog/vast-v2.3), we have automatic rebuilding in
place, which merges underfull partitions to reduce pressure on the central
catalog.

How does the number of events relates to their size?

```{r}
#| code-fold: true
#| label: log-log-event-memory
no_segment |>
  ggplot(aes(x = num_events, y = bytes_memory, color = store)) +
    geom_smooth(method = "lm", se = FALSE, size = 0.5) +
    geom_point(alpha = 0.5) +
    guides(color = "none") +
    scale_y_log10(labels = scales::label_bytes(units = "auto_si")) +
    scale_x_log10(labels = scales::label_comma()) +
    labs(color = "Store", x = "Events", y = "Bytes (in memory)") +
    facet_grid(~ store)
```

The above [log-log plot](https://en.wikipedia.org/wiki/Log%E2%80%93log_plot)
includes a fitted linear model. Except for the tails, we can see that the points
align on a straight line. This means that there exists a power-function
relationship between the number of events and the store size. The slope of the
line is the exponent and precisely captures this relationship.

For the remainder of the analysis, we look at the normalized event size, by
dividing the store size by the number of contained events.

### Schemas

We have a total of `r length(unique(data$schema))` unique schemas:

```{r}
#| label: schemas
#| echo: false
unique(data$schema)
```

The schemas belong to three data types: Zeek, Suricata, and PCAP. These are all
data sources VAST can ingest natively. Zeek and Suricata are text, whereas PCAP
is a binary format representing a stream of network packets.

Our dataset has multiple different schemas, each of which has a different number
of events:

```{r}
#| code-fold: true
#| label: number-of-events-by-schema
schemas <- normalized |>
  filter(store == "feather") |>
  group_by(schema) |>
  summarize(n = median(num_events),
            bytes_memory = median(bytes_memory))

schemas |>
  separate(schema, c("module", "type"), remove = FALSE) |>
  ggplot(aes(x = reorder(schema, -n), y = n, fill = module)) +
    geom_bar(stat = "identity") +
    guides(fill = "none") +
    scale_y_log10(labels = scales::label_comma()) +
    labs(x = "Schema", y = "Number of Events", color = "Module") +
    theme(axis.text.x = element_text(angle = 90, size = 8, vjust = 0.5, hjust = 1))
```

Zeek connection logs lead the pack, followed by PCAP packets. Some more
voluminous events represent the long tail, e.g., `suricata.ikev2`. What this
shows is basically that we have a diverse dataset. We already saw the
relationship between the number of events and their (in-memory) size on the
log-log plot earlier, but if we exchange number of events with their size, we
can observe almost the same order:

```{r}
#| code-fold: true
#| label: event-size-by-schema
schemas |>
  separate(schema, c("module", "type"), remove = FALSE) |>
  ggplot(aes(x = reorder(schema, -n), y = bytes_memory, fill = module)) +
    geom_bar(stat = "identity") +
    guides(fill = "none") +
    scale_y_log10(labels = scales::label_bytes(units = "auto_si")) +
    labs(x = "Schema", y = "Bytes (in-memory)", color = "Module") +
    theme(axis.text.x = element_text(angle = 90, size = 8, vjust = 0.5, hjust = 1))
```

## Size

To better understand the difference between Parquet and Feather, we now take a
look at them right next to each other. In addition to Feather and Parquet, we
use three other types of "stores" for the analysis to faciliate comparison:

1. **Original**: the size of the input prior it entered VAST, e.g., the raw JSON or
   a PCAP file.

2. **Segment**: our custom store prior to having switched to Feather. We omit
   this reference point often, as it is nearly equivalent to Feather.

3. **Memory**: the size of the data in memory, measured as the sum of Arrow
   buffers that make up the table slice.

Let's kick of the analysis by getting a better understanding at the distribution
of size.

```{r}
#| code-fold: true
#| label: plot-schema-distribution-boxplot
unified |>
  bind_rows(original, memory) |>
  ggplot(aes(x = reorder(store, -bytes, FUN = "median"),
             y = bytes, color = store_class)) +
  geom_boxplot() +
  scale_y_log10(labels = scales::label_bytes(units = "auto_si")) +
  labs(x = "Store", y = "Bytes/Event", color = "Store")
```

Every boxplot corresponds to one store, with `original` and `memory` being also
treated like stores. The suffix `-Z` indicates Zstd compression level `Z`, with
`NA` meaning "compression turned off" entirely.  Parquet stores on the right (in
purple) have the smallest size, followed by Feather (red), and then their
corresponding in-memory (green) and original (turquoise) representation. The
negative Zstd compression level -5 makes Parquet actually worse than Feather.

What stands out is that disabling compression for Feather inflates the data
larger than the original. This is not the case for Parquet. Why? Because Parquet
has an orthogonal layer of compression using dictionaries. This absorbs
inefficiencies in heavy-tailed distributions, which are pretty standard in
machine-generated data.

The y-axis of above plot is log-scaled, which makes it hard for relative
comparison. Let's focus on the medians (the bars in the box) only and bring the
y-axis to a linear scale:

```{r}
#| code-fold: true
#| label: plot-schema-distribution-medians
medians <- unified |>
  bind_rows(original, memory) |>
  group_by(store, store_class) |>
  summarize(bytes = median(bytes))

medians |>
  ggplot(aes(x = reorder(store, -bytes), y = bytes, fill = store_class)) +
  geom_bar(stat = "identity") +
  scale_y_continuous(labels = scales::label_bytes(units = "auto_si")) +
  labs(x = "Store", y = "Bytes/Event", fill = "Store")
```

To better understand the compression in numbers, we'll anchor the original size
at 100% and now show the relative gains:

```{r}
#| echo: false
library(knitr)

median_original <- medians |> filter(store == "original") |> pull(bytes)
medians |>
  filter(store_class %in% c("feather", "parquet")) |>
  mutate(size = bytes / median_original * 100) |>
  mutate(compression = median_original / size) |>
  arrange(size) |>
  kable(digits = 1)
```

The above analysis covered averages across schemas. If we juxtapose Parquet and
Feather per schema, we see the difference between the two formats more clearly:

```{r}
#| code-fold: true
#| label: plot-parquet-vs-feather
library(ggrepel)
library(stringr)

parquet_vs_feather <- unified |>
  select(-store, -duration) |>
  pivot_wider(names_from = store_class,
              values_from = bytes,
              id_cols = c(schema, zstd.level))

plot_parquet_vs_feather <- function(data) {
  data |>
    mutate(zstd.level = str_replace_na(zstd.level)) |>
    separate(schema, c("module", "type"), remove = FALSE) |>
    ggplot(aes(x = parquet, y = feather,
               shape = zstd.level, color = zstd.level)) +
      geom_abline(intercept = 0, slope = 1, color = "grey") +
      geom_point(alpha = 0.6) +
      geom_text_repel(aes(label = schema),
                color = "grey",
                size = 1, # font size
                box.padding = 0.2,
                min.segment.length = 0, # draw all line segments
                max.overlaps = Inf,
                segment.color = "grey",
                segment.alpha = 0.3) +
      scale_size(range = c(0, 10)) +
      labs(x = "Bytes (Parquet)", y = "Bytes (Feather)",
           shape = "Compression\nLevel", color = "Compression\nLevel")
}

parquet_vs_feather |>
  plot_parquet_vs_feather() +
    scale_x_log10(labels = scales::label_bytes(units = "auto_si")) +
    scale_y_log10(labels = scales::label_bytes(units = "auto_si"))
```

In the above log-log scatterplot, the straight line is the identity function.
Each point represents the median store size for a given schema. If a point is on
the line, it means there is no difference between Feather and Parquet. The color
and shape shows the different compression levels, with `NA` meaning no
compression. Points clouds closer to the origin mean that the corresponding
store class takes up less space.

As above, disabling compression hits Feather the hardest. The red circles
(compression level -5) also stand out as a poor choice. Otherwise it's hard to
see noticeable differences.

Points below the identity line would represent a counter-intuitive behavior
where Feather is smaller than Parquet. This is an artifact of our dataset.
Basically anything above the 1 kB region is also distorted, because we have
sometimes just a single event. Normalizing the store size by the number of
events fails here, as there is a constant-size overhead for the Parquet framing
that is higher than Feather. We need a certain number of events for this to
amortize.

If we zoom in on the region <= 100 bytes and remove the log scaling, things
become a bit more readable:

```{r}
#| code-fold: true
#| label: plot-parquet-vs-feather-100
parquet_vs_feather |>
  filter(feather <= 100) |>
  plot_parquet_vs_feather() +
    scale_x_continuous(labels = scales::label_bytes(units = "auto_si")) +
    scale_y_continuous(labels = scales::label_bytes(units = "auto_si"))
```

While interesting to see that some schemas compress better than others, the
additional insight of this display is marginal.

```{r}
#| code-fold: true
#| label: plot-zeek-suricata
zeek_conn <- unified |>
  filter(store %in% c("parquet", "feather") & schema == "zeek.conn") |>
  pull(bytes)

suricata_snmp <- unified |>
  filter(store %in% c("parquet", "feather") & schema == "suricata.snmp") |>
  pull(bytes)

suricata_ikev2 <- unified |>
  filter(store %in% c("parquet", "feather") & schema == "suricata.ikev2") |>
  pull(bytes)

unified |>
  filter(schema %in% c("zeek.conn", "suricata.snmp", "suricata.ikev2")) |>
  ggplot(aes(x = reorder(store, -bytes), y = bytes, fill = store)) +
    geom_bar(stat = "identity") +
    guides(fill = "none") +
    labs(x = "Store", y = "Median Bytes/Event") +
    scale_y_continuous(labels = scales::label_bytes(units = "auto_si")) +
    theme(axis.text.x = element_text(angle = 90, size = 8, vjust = 0.5, hjust = 1)) +
    facet_wrap(~ schema, scales = "free")
```

For `zeek.conn`, the percentage that Parquet takes compared to Feather is
`r scales::label_percent()(zeek_conn[2] / zeek_conn[1])`, whereas it is
`r scales::label_percent()(suricata_snmp[2] / suricata_snmp[1])` for
`suricata.snmp`.

:::caution TODO
Thomas, we need to look at suricata.ikev2. It's really weird to see the
blow-up.
:::

We finally look at the fraction of space Parquet takes compared to Feather on a
per schema basis:

```{r}
#| code-fold: true
#| label: plot-parquet-divided-by-feather
library(tibble)

parquet_vs_feather |>
  filter(feather <= 100) |>
  mutate(zstd.level = str_replace_na(zstd.level)) |>
  ggplot(aes(x = reorder(schema, -parquet / feather),
             y = parquet / feather,
             fill = zstd.level)) +
    geom_hline(yintercept = 1) +
    geom_bar(stat = "identity", position = "dodge") +
    labs(x = "Schema", y = "Parquet / Feather (%)", fill = "Compression\nLevel") +
    scale_y_continuous(breaks = 6:1 * 20 / 100, labels = scales::label_percent()) +
    theme(axis.text.x = element_text(angle = 90, size = 8, vjust = 0.5, hjust = 1))
```

Overall, we see that that compression level 1 and 9 do not have big differene,
but 19 brings Feather and Parquet closer together for half of the schemas.

## Write Throughput

With write throughput, we mean the time it takes to transform Arrow Record
Batches into Parquet and Feather format.

```{r}
#| code-fold: true
#| label: compute-bytes-per-event
unified |>
  ggplot(aes(x = reorder(store, -duration, FUN = "median"),
             y = duration, color = store_class)) +
  geom_boxplot() +
  scale_y_log10(labels = scales::label_number(scale = 1e6, suffix = "us")) +
  labs(x = "Store", y = "Duration (us)", color = "Store")
```

The above boxplots have a log-scaled y-axis and are sorted by their median,
similar to the size discussion above.

```{r}
unified |>
  filter(schema %in% c("zeek.conn", "suricata.snmp", "suricata.ikev2")) |>
  ggplot(aes(x = reorder(store, -duration), y = duration, fill = store)) +
    geom_bar(stat = "identity") +
    guides(fill = "none") +
    labs(x = "Store", y = "Median Duration/Event") +
    scale_y_continuous(labels = scales::label_number(scale = 1e3, suffix = "ms")) +
    theme(axis.text.x = element_text(angle = 90, size = 8, vjust = 0.5, hjust = 1)) +
    facet_wrap(~ schema, scales = "free")
```
